{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#from functools32 import lru_cache\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "def tokens(sentence):\n",
    "  return word_tokenize(sentence)\n",
    "\n",
    "def lemmatize(tokens):\n",
    "  wnl = WordNetLemmatizer()\n",
    "  return [wnl.lemmatize(token) for token in tokens]\n",
    "\n",
    "def postags(tokens):\n",
    "  return nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Daniel']\n",
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Daniel', 'NNP')]\n",
      "../STS-data/STS2012-gold/STS.input.MSRpar.txt\n",
      "../STS-data/STS2012-gold/STS.input.MSRvid.txt\n",
      "../STS-data/STS2012-gold/STS.input.SMTeuroparl.txt\n",
      "../STS-data/STS2012-gold/STS.input.surprise.OnWN.txt\n",
      "../STS-data/STS2012-gold/STS.input.surprise.SMTnews.txt\n",
      "../STS-data/STS2012-train/STS.input.MSRpar.txt\n",
      "../STS-data/STS2012-train/STS.input.MSRvid.txt\n",
      "../STS-data/STS2012-train/STS.input.SMTeuroparl.txt\n",
      "../STS-data/STS2013-gold/STS.input.FNWN.txt\n",
      "../STS-data/STS2013-gold/STS.input.headlines.txt\n",
      "../STS-data/STS2013-gold/STS.input.OnWN.txt\n",
      "../STS-data/STS2014-gold/STS.input.deft-forum.txt\n",
      "../STS-data/STS2014-gold/STS.input.deft-news.txt\n",
      "../STS-data/STS2014-gold/STS.input.headlines.txt\n",
      "../STS-data/STS2014-gold/STS.input.images.txt\n",
      "../STS-data/STS2014-gold/STS.input.OnWN.txt\n",
      "../STS-data/STS2014-gold/STS.input.tweet-news.txt\n",
      "../STS-data/STS2015-gold/STS.input.answers-forums.txt\n",
      "../STS-data/STS2015-gold/STS.input.answers-students.txt\n",
      "../STS-data/STS2015-gold/STS.input.belief.txt\n",
      "../STS-data/STS2015-gold/STS.input.headlines.txt\n",
      "../STS-data/STS2015-gold/STS.input.images.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.answer-answer.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.answer-answer.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.headlines.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.headlines.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.plagiarism.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.plagiarism.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.postediting.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.postediting.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.question-question.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.question-question.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Is it inappropriate to spend personal resources for the company's benefit?\",\n",
       " \"Is it ethical/legal to use the University's resources for a personal project?\",\n",
       " '\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"My name is Daniel\"\n",
    "word_tokens = tokens(sentence)\n",
    "print lemmatize(word_tokens)\n",
    "print postags(word_tokens)\n",
    "\n",
    "import glob\n",
    "import re\n",
    "\n",
    "fname = \"../STS-data/STS2013-gold/STS.input.headlines.txt\"\n",
    "with open(fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "Data = []\n",
    "    \n",
    "for fname in glob.glob(\"../STS-data/*201[1-5]*/*input*.txt\"):\n",
    "    print fname\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.split('\\t')) !=2 :\n",
    "                print fname\n",
    "                break;\n",
    "            else:\n",
    "                Data.append(line.split('\\t'))\n",
    "for fname in glob.glob(\"../STS-data/*2016*/*input*.txt\"):\n",
    "    print fname\n",
    "    print re.sub(\"input\",\"gs\",fname)\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "        with open(re.sub(\"input\",\"gs\",fname)) as nfile:\n",
    "            nlines = nfile.readlines()\n",
    "        #nlines[index]\n",
    "        for index,line in enumerate(lines):\n",
    "            if len(line.split('\\t')) !=4 :\n",
    "                #print fname\n",
    "                break;\n",
    "            else:\n",
    "                x = line.split('\\t')\n",
    "                Data.append([x[0],x[1],nlines[index]])\n",
    "Data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = []\n",
    "fname = \"../STS-data/STS2013-gold/STS.input.headlines.txt\"\n",
    "with open(fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(re.sub(\"input\",\"gs\",fname)) as nfile:\n",
    "        nlines = nfile.readlines()\n",
    "        for index,line in enumerate(lines):\n",
    "            if len(line.split('\\t')) !=2 :\n",
    "                #print fname\n",
    "                break;\n",
    "            else:\n",
    "                if(re.search(\"(\\d)((\\.)(\\d))?\", nlines[index])):\n",
    "                    ;#print nlines[index]\n",
    "                else:\n",
    "                    print lines[index]\n",
    "                    break\n",
    "                x = line.split('\\t')\n",
    "                Data.append([x[0],x[1], float(re.sub(\"\\n\",\"\",nlines[index]))])\n",
    "                \n",
    "#print Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = []\n",
    "fname = \"../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.question-question.txt\"\n",
    "with open(fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(re.sub(\"input\",\"gs\",fname)) as nfile:\n",
    "        nlines = nfile.readlines()\n",
    "        #print nlines\n",
    "        \n",
    "        for index,line in enumerate(lines):\n",
    "            if len(line.split('\\t')) !=4 :\n",
    "                #print fname\n",
    "                break;\n",
    "            else:\n",
    "                if(not re.search(\"(\\d)((\\.)(\\d))?\", nlines[index])):\n",
    "                    continue;#print nlines[index]\n",
    "                #else:\n",
    "                    #print lines[index]\n",
    "                    #continue\n",
    "                x = line.split('\\t')\n",
    "                Data.append([x[0],x[1], float(re.sub(\"\\n\",\"\",nlines[index]))])\n",
    "                \n",
    "#print Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import preprocess as process\n",
    "import utilities as utility\n",
    "import math\n",
    "\n",
    "def ngram_vector_keys(tokens, ngram_size=1):\n",
    "  vector_keys = []\n",
    "  for i in range(len(tokens)-(ngram_size-1)):\n",
    "    vector_keys.append(tuple(tokens[i:i+(ngram_size)]))\n",
    "    # vector_keys.append(\" \".join(tokens[i:i+(ngram_size)]))\n",
    "  return vector_keys\n",
    "\n",
    "def containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1):\n",
    "  set1 = set(ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "  set2 = set(ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  containment_of_sentence_1_in_2 = float(len(set1.intersection(set2)))/len(set1)\n",
    "  containment_of_sentence_2_in_1 = float(len(set1.intersection(set2)))/len(set2)\n",
    "  return containment_of_sentence_1_in_2, containment_of_sentence_2_in_1\n",
    "\n",
    "def JaccardCoefficient(sent_1_tokens, sent_2_tokens, ngram_size=1):\n",
    "  set1 = set(ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "  set2 = set(ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  #print set1,set2\n",
    "  return float(len(set1.intersection(set2)))/len(set1.union(set2))\n",
    "\n",
    "def TFIDF(documents):\n",
    "  Vocabulary = Counter()\n",
    "  DocVectors = []\n",
    "  IDFVector = Counter()\n",
    "  No_of_Documents = float(len(documents))\n",
    "  for document in documents:\n",
    "    tf_single_doc_count = Counter(process.tokens(document))\n",
    "    Vocabulary+= tf_single_doc_count\n",
    "    DocVectors.append(tf_single_doc_count)\n",
    "    IDFVector += Counter(tf_single_doc_count.keys())\n",
    "  # print IDFVector\n",
    "  for key in IDFVector.keys():\n",
    "    IDFVector[key] = math.log(No_of_Documents/(1+IDFVector[key]))\n",
    "  # print IDFVector\n",
    "  # IDFVector = defaultdict(lambda:0.0, dict((key,Vocabulary[key]*) for key in c.keys()))\n",
    "  TFIDFScores = defaultdict(lambda:0.0, dict((key,Vocabulary[key]*IDFVector[key]) for key in Vocabulary.keys()))\n",
    "  # print TFIDFScores, IDFVector, Vocabulary\n",
    "  return TFIDFScores, Vocabulary, DocVectors, IDFVector\n",
    "\n",
    "def DocvectorTFIDF(TFIDFScores, tokens):\n",
    "  return defaultdict(lambda:0.0, dict((key,TFIDFScores[key] if key in TFIDFScores.keys() else 0.0) for key in tokens))\n",
    "\n",
    "\n",
    "def cosinesimilarity(document1, document2, TFIDFScores):\n",
    "  tokens1 = set(process.tokens(document1))\n",
    "  tokens2 = set(process.tokens(document2))\n",
    "  vector1 = DocvectorTFIDF(TFIDFScores, tokens1)\n",
    "  vector2 = DocvectorTFIDF(TFIDFScores, tokens2)\n",
    "  len_vector_1 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_1\n",
    "  len_vector_2 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_2\n",
    "  print utility.dict_dotprod(vector1,vector2)\n",
    "  cosine_similarity_score = (utility.dict_dotprod(vector1,vector2))/float((len_vector_2*len_vector_1))\n",
    "  return cosine_similarity_score\n",
    "\n",
    "\n",
    "def cosinesimilarity_without_TFIDF(document1, document2):\n",
    "  vector1 = Counter(process.tokens(document1))\n",
    "  vector2 = Counter(process.tokens(document2))\n",
    "  print vector1\n",
    "  print vector2\n",
    "  len_vector_1 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_1\n",
    "  len_vector_2 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_2\n",
    "  cosine_similarity_score = (utility.dict_dotprod(vector1,vector2))/float((len_vector_2*len_vector_1))\n",
    "  return cosine_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41421356237\n",
      "1.41421356237\n",
      "1.0\n",
      "0.5\n",
      "Counter({'i': 1, 'ike': 1})\n",
      "Counter({'i': 1, 'ike': 1})\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.0\n",
      "(1.0, 0.6)\n",
      "0.6\n",
      "0.5\n",
      "0.428571428571\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "TFIDFScores, Vocabulary, DocVectors, IDFVector = TFIDF([\"i ike\", \"i ike\"])\n",
    "TFIDFScores = {\"i\":1,\"ike\":1}\n",
    "v = DocvectorTFIDF(TFIDFScores, process.tokens(\"i ike\"))\n",
    "#print v\n",
    "#print utility.dict_dotprod(v,DocvectorTFIDF(TFIDFScores, process.tokens(\"i tes\")))\n",
    "print cosinesimilarity(\"i ike\", \"i tes\", {\"i\":1,\"ike\":1})\n",
    "print cosinesimilarity_without_TFIDF(\"i ike\", \"i ike\")\n",
    "\n",
    "JaccardCoefficient([\"sent_1_tokens\",\"test\"], [\"sent_2_tokens\",\"tes\",\"Tesd\",\"Tesd\",\"Tesd\"], ngram_size=2)\n",
    "print containment_coefficienct([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=2)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=3)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
